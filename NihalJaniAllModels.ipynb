{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "data = pd.read_csv('processed_iot_data.csv')\n",
    "\n",
    "features = data.drop(columns=['label']).values\n",
    "labels = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the IoTDataset class\n",
    "class IoTDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and test sets\n",
    "train_dataset = IoTDataset(X_train, y_train)\n",
    "test_dataset = IoTDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define LSTM Model\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(1)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        energy = torch.tanh(self.attn(encoder_outputs))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        energy = torch.bmm(v, energy)\n",
    "        return energy.squeeze(1)\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.5):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        attn_weights = self.attention(h_n[-1], lstm_out)\n",
    "        context = attn_weights.bmm(lstm_out)\n",
    "        context = context.squeeze(1)\n",
    "        context = self.dropout(context)  # Apply dropout\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256  # Increased hidden size\n",
    "output_size = 1\n",
    "dropout = 0.5  # Dropout rate\n",
    "learning_rate = 0.0001  # Lower learning rate\n",
    "batch_size = 64  # Increased batch size\n",
    "\n",
    "# Model initialization\n",
    "model = LSTMWithAttention(input_size, hidden_size, output_size, dropout)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(20):  # Increase number of epochs\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)  # Ensure inputs are of shape (batch_size, sequence_length, input_size)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'lstm_with_attention_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8943932605540481\n",
      "Precision: 0.8978575851393189\n",
      "Recall: 0.7418981836786902\n",
      "F1 Score: 0.8124611290096653\n",
      "Specificity: 0.9623748374883105\n",
      "ROC AUC: 0.9474732139850601\n",
      "Confusion Matrix:\n",
      " [[210967   8248]\n",
      " [ 25223  72502]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def print_metrics(y_true, y_pred, y_pred_prob):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_prob = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.unsqueeze(1)  # Ensure inputs are of shape (batch_size, sequence_length, input_size)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs.squeeze())\n",
    "            predictions = torch.round(probs)\n",
    "            y_true.extend(targets.tolist())\n",
    "            y_pred.extend(predictions.tolist())\n",
    "            y_pred_prob.extend(probs.tolist())\n",
    "    return y_true, y_pred, y_pred_prob\n",
    "\n",
    "# Load the trained model\n",
    "model = LSTMWithAttention(input_size, hidden_size, output_size, dropout)\n",
    "model.load_state_dict(torch.load('lstm_with_attention_model.pth'))\n",
    "\n",
    "# Evaluate the model and print metrics\n",
    "y_true, y_pred, y_pred_prob = evaluate(model, test_loader)\n",
    "print_metrics(y_true, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6915909647941589\n",
      "Epoch 2, Loss: 0.25700071454048157\n",
      "Epoch 3, Loss: 0.1986079216003418\n",
      "Epoch 4, Loss: 0.33182841539382935\n",
      "Epoch 5, Loss: 0.4346599578857422\n",
      "Epoch 6, Loss: 0.22414802014827728\n",
      "Epoch 7, Loss: 0.2908736765384674\n",
      "Epoch 8, Loss: 0.358585000038147\n",
      "Epoch 9, Loss: 0.23139211535453796\n",
      "Epoch 10, Loss: 0.39025577902793884\n",
      "Epoch 11, Loss: 0.1071011945605278\n",
      "Epoch 12, Loss: 0.23711292445659637\n",
      "Epoch 13, Loss: 0.12072049081325531\n",
      "Epoch 14, Loss: 0.4184686839580536\n",
      "Epoch 15, Loss: 0.10816724598407745\n",
      "Epoch 16, Loss: 0.3082788288593292\n",
      "Epoch 17, Loss: 0.08810606598854065\n",
      "Epoch 18, Loss: 0.232331782579422\n",
      "Epoch 19, Loss: 0.1703435778617859\n",
      "Epoch 20, Loss: 0.12610751390457153\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "data = pd.read_csv('processed_iot_data.csv')\n",
    "\n",
    "features = data.drop(columns=['label']).values\n",
    "labels = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizing features to have mean of 0 and variance of 1\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Defining IoTDataset class for easy data loading to model\n",
    "class IoTDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# Dataloader for easy batch processing\n",
    "train_dataset = IoTDataset(X_train, y_train)\n",
    "test_dataset = IoTDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Defining the Self-Attention mechanism for LSTM with given hidden size, using softmax function, and standard \n",
    "# input forwarding functionality\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_scores = self.attention(lstm_output)  \n",
    "        attention_weights = self.softmax(attention_scores)  \n",
    "        weighted_output = torch.bmm(attention_weights.transpose(1, 2), lstm_output)  \n",
    "        return weighted_output.sum(dim=1)  \n",
    "\n",
    "# Defining the LSTM model with integrated Self-Attention mecahnism, with standard forwarding functionality, and \n",
    "# standard dropout regularization so model doesn't overfit and generalizes well to the unseen data\n",
    "class LSTMWithSelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.5):\n",
    "        super(LSTMWithSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.self_attention = SelfAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)   # Mapping output the output from the self-attention mechanism to the final output\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        attention_output = self.self_attention(lstm_out)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        out = self.fc(attention_output)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Model initialization\n",
    "model = LSTMWithSelfAttention(input_size, hidden_size, output_size, dropout)\n",
    "\n",
    "# Applying BCE with Logits Loss function for binary classification, computing loss \n",
    "# based on 'label's.criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(20):  \n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)  \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Saving the trained model for metric scores \n",
    "torch.save(model.state_dict(), 'lstm_with_self_attention_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8942544330157127\n",
      "Precision: 0.8921077700969735\n",
      "Recall: 0.7474443591711435\n",
      "F1 Score: 0.8133939856238481\n",
      "Specificity: 0.9597016627511803\n",
      "ROC AUC: 0.9479894074234172\n",
      "Confusion Matrix:\n",
      " [[210381   8834]\n",
      " [ 24681  73044]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def print_metrics(y_true, y_pred, y_pred_prob):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_prob = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.unsqueeze(1)  \n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs.squeeze())\n",
    "            predictions = torch.round(probs)\n",
    "            y_true.extend(targets.tolist())\n",
    "            y_pred.extend(predictions.tolist())\n",
    "            y_pred_prob.extend(probs.tolist())\n",
    "    return y_true, y_pred, y_pred_prob\n",
    "\n",
    "model = LSTMWithSelfAttention(input_size, hidden_size, output_size, dropout)\n",
    "model.load_state_dict(torch.load('lstm_with_self_attention_model.pth'))\n",
    "\n",
    "y_true, y_pred, y_pred_prob = evaluate(model, test_loader)\n",
    "print_metrics(y_true, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Merging the benign and spoofing datasets\n",
    "data = pd.read_csv(\"./processed_iot_data.csv\")\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "features = data.drop(columns=['label']).values\n",
    "labels = data['label'].values\n",
    "\n",
    " #Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print_metrics(y_test, y_pred, y_pred_prob)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 300), \n",
    "    'max_depth': randint(5, 20), \n",
    "    'min_samples_split': randint(2, 10), \n",
    "    'min_samples_leaf': randint(1, 10),  \n",
    "    'bootstrap': [True, False]  \n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize Randomized Search for finidng best hyper-parameters\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, \n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    scoring='accuracy',  # Scoring metric to evaluate the model\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=0,  # Set to 0 to use custom tqdm tracker\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Wrap the fit method with tqdm progress bar\n",
    "class TqdmCallback:\n",
    "    def __init__(self, total):\n",
    "        self.progress_bar = tqdm(total=total)\n",
    "\n",
    "    def __call__(self, n):\n",
    "        self.progress_bar.update(n)\n",
    "\n",
    "n_iter = random_search.n_iter\n",
    "callback = TqdmCallback(total=n_iter)\n",
    "\n",
    "# Wrap fit function with the progress bar\n",
    "for i in tqdm(range(n_iter), desc=\"Tuning progress\"):\n",
    "    random_search.fit(X_train, y_train)\n",
    "    callback(1)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy Score:\", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print metrics or any other analysis you want to perform\n",
    "print_metrics(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "y_pred_prob = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"XGBoost Classifier:\")\n",
    "print_metrics(y_test, y_pred, y_pred_prob)\n",
    "\n",
    "# Grid Search with XGBoost\n",
    "param_grid = {\n",
    "    'learning_rate': [1.0, 0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "}\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_classifier = XGBClassifier(**best_params)\n",
    "best_xgb_classifier.fit(X_train, y_train)\n",
    "y_pred = best_xgb_classifier.predict(X_test)\n",
    "y_pred_prob = best_xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"XGBoost Classifier with Grid Search:\")\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print_metrics(y_test, y_pred, y_pred_prob)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
